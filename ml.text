
ğŸ§© PRACTICAL â€” K-Means Clustering on Sales Data
Problem:
	Implement K-Means clustering on sales_data_sample.csv and determine the number of clusters using the Elbow Method.

ğŸ§  Cell 1: Import Required Libraries
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.cluster import KMeans
import warnings
from sklearn.preprocessing import StandardScaler
warnings.filterwarnings('ignore')
ğŸ” Explanation:
	â€¢ pandas â†’ used to load and handle dataset (CSV files, tables).
	â€¢ matplotlib.pyplot â†’ for plotting graphs and visualizations.
	â€¢ KMeans â†’ the main machine learning algorithm for clustering.
	â€¢ StandardScaler â†’ used to scale (normalize) data before clustering.
	â€¢ warnings.filterwarnings('ignore') â†’ hides unnecessary warning messages while running code.
ğŸ§© Purpose:
These libraries are like â€œtoolsâ€ you import before doing your analysis.

ğŸ§  Cell 2: Load Dataset
df = pd.read_csv('./Datasets/sales_data_sample.csv', encoding='ISO-8859-1')
df
ğŸ” Explanation:
	â€¢ pd.read_csv() â†’ reads the CSV file and stores it into a DataFrame called df.
	â€¢ './Datasets/sales_data_sample.csv' â†’ path to your dataset.
	â€¢ encoding='ISO-8859-1' â†’ used because the dataset may contain special characters (like in customer names).
	â€¢ Writing only df in a Jupyter cell displays the first few rows.
ğŸ§© Purpose:
To load your dataset into memory so you can work with it.

ğŸ§  Cell 3: Understand the Data
df.describe()
df.info()
ğŸ” Explanation:
	â€¢ df.describe() â†’ gives statistical details (mean, min, max, etc.) for numeric columns.
Example output:
QUANTITYORDERED | PRICEEACH | SALES | ...
count  mean  std  min  max

	â€¢ df.info() â†’ shows column names, data types (int, float, object), and number of missing values.
ğŸ§© Purpose:
You get to know what kind of data you have â€” numbers, text, missing values, etc.

ğŸ§  Cell 4: Select Columns for Clustering
df = df[['ORDERLINENUMBER', 'SALES']]
ğŸ” Explanation:
	â€¢ The dataset has many columns (like customer name, product line, etc.), but you only select numeric columns that make sense for clustering.
	â€¢ Here, you choose:
		â—‹ ORDERLINENUMBER â†’ the sequence number of an item in an order.
		â—‹ SALES â†’ total sales amount.
ğŸ§© Purpose:
Clustering needs numbers, not text â€” youâ€™re focusing on two measurable variables.

ğŸ§  Cell 5: Scale (Normalize) the Data
scaler = StandardScaler()
scaled_values = scaler.fit_transform(df.values)
ğŸ” Explanation:
	â€¢ StandardScaler() standardizes data so that each column has:
		â—‹ Mean = 0
		â—‹ Standard deviation = 1
	â€¢ fit_transform() both learns the scaling (fit) and applies it (transform).
	â€¢ df.values extracts the raw numeric values from the DataFrame.
ğŸ§© Purpose:
Scaling ensures that large numbers like SALES donâ€™t overpower smaller ones like ORDERLINENUMBER.
K-Means uses distance â€” so all features should be on the same scale.

ğŸ§  Cell 6: Elbow Method (Find Best K)
wcss = []
for i in range(1, 11):
    model = KMeans(n_clusters=i, init='k-means++')
    model.fit_predict(scaled_values)
    wcss.append(model.inertia_)
plt.plot(range(1, 11), wcss, 'ro-')
plt.show()
ğŸ” Explanation:
	â€¢ wcss = [] â†’ create an empty list to store inertia values (Within Cluster Sum of Squares).
	â€¢ for i in range(1, 11) â†’ test for different cluster counts (K = 1 to 10).
	â€¢ KMeans(n_clusters=i, init='k-means++') â†’ creates a KMeans model with i clusters.
		â—‹ init='k-means++' helps start with smarter centroids.
	â€¢ fit_predict() â†’ runs the model and assigns each data point to a cluster.
	â€¢ model.inertia_ â†’ stores the total distance between data points and their nearest cluster centers (WCSS).
	â€¢ Append each inertia to the list wcss.
	â€¢ Plot K vs WCSS â†’ shows how inertia changes with K.
ğŸ§© Purpose:
You are testing how many clusters make sense by observing where the curve bends (the "elbow").

ğŸ§  Cell 7: Apply K-Means with Chosen K
model = KMeans(n_clusters=4, init='k-means++')
clusters = model.fit_predict(scaled_values)
df['Cluster'] = clusters
ğŸ” Explanation:
	â€¢ n_clusters=4 â†’ from the Elbow graph, you decided K = 4 gives the best result.
	â€¢ fit_predict() â†’ runs the K-Means algorithm:
		1. Finds 4 cluster centers.
		2. Assigns each record to the nearest cluster.
	â€¢ df['Cluster'] = clusters â†’ adds a new column in your dataset to label each record with its cluster number (0,1,2,3).
ğŸ§© Purpose:
Actually perform the clustering and record which cluster each data point belongs to.

ğŸ§  Cell 8: Check Inertia Value
model.inertia_
ğŸ” Explanation:
	â€¢ Displays the final inertia (WCSS) for K=4.
	â€¢ Example output: 1709.25
	â€¢ This shows how tight your clusters are.
ğŸ§© Purpose:
To measure how good the clustering is â€” smaller = better (but not always lowest, as explained in Elbow Method).

ğŸ§  Cell 9: Visualize Clusters
plt.scatter(df['ORDERLINENUMBER'], df['SALES'], c=model.labels_)
plt.show()
ğŸ” Explanation:
	â€¢ plt.scatter() â†’ plots each point based on ORDERLINENUMBER (x-axis) and SALES (y-axis).
	â€¢ c=model.labels_ â†’ colors each point based on its cluster.
	â€¢ plt.show() â†’ displays the scatter plot.
ğŸ§© Purpose:
To visually see how the data points are grouped into clusters.

ğŸ§¾ Summary of Each Step
Step	Code	What It Does
1	import ...	Load necessary libraries
2	pd.read_csv()	Load dataset
3	df.info()	Check data summary
4	Select columns	Keep only numeric data
5	StandardScaler()	Normalize data
6	Loop with KMeans	Apply Elbow Method
7	KMeans(n_clusters=4)	Perform clustering
8	model.inertia_	Check compactness of clusters
9	plt.scatter()	Visualize the clusters

ğŸ§® How the Algorithm Works (Conceptually)
	1. Choose K = 4
	2. Randomly select 4 points as initial centroids.
	3. Assign data points to the nearest centroid (based on distance).
	4. Recalculate centroids (mean position of points in each cluster).
	5. Repeat until centroids stop changing.

ğŸ¤ Viva Questions (with Answers)
Question	Answer
What is K-Means?	Itâ€™s an unsupervised algorithm that groups data into K clusters based on distance.
What is the Elbow Method?	A graphical method to find the best K by plotting K vs WCSS.
What is inertia?	The total distance of all points to their nearest cluster center.
What does fit_predict() do?	Fits the model and assigns each point to a cluster.
Why do we use StandardScaler?	To normalize all features so they have equal importance.
What is init='k-means++'?	A smart way to choose starting centroids for faster convergence.
How did you decide K?	From the elbow curve where inertia starts to flatten (K=4).
What is unsupervised learning?	A type of learning without labels, where we find patterns in data.
What does each color in scatter plot show?


	Each color represents a different cluster.
ğŸ§© PRACTICAL 4 â€” K-Nearest Neighbors (KNN) on Diabetes Dataset
Problem Statement:
        Implement KNN algorithm on diabetes.csv dataset and compute confusion matrix, accuracy, error rate, precision, and recall.

ğŸ§  Step 1: Import Necessary Libraries
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, classification_report
ğŸ“˜ Explanation:
Library
Purpose
pandas
Used to load and handle datasets (CSV files, tables).
numpy
Used for mathematical operations.
train_test_split
Splits your data into training and testing sets.
StandardScaler
Scales (normalizes) numeric data so all features are on the same scale.
KNeighborsClassifier
KNN algorithm â€” used for classification (predicts the class).
metrics
Used to evaluate how well your model performs (accuracy, confusion matrix, etc.).
ğŸ§© Purpose:
Before you start machine learning, you must import all the tools you need.

ğŸ§  Step 2: Load the Dataset
df = pd.read_csv("diabetes.csv")
ğŸ“˜ Explanation:
        â€¢ Reads the dataset named diabetes.csv into a pandas DataFrame named df.
        â€¢ Each row represents a patient, and columns represent medical measurements like:
                â—‹ Pregnancies, Glucose, BloodPressure, Insulin, BMI, etc.
        â€¢ The column Outcome (0 or 1) is the target variable, where:
                â—‹ 0 â†’ No diabetes
                â—‹ 1 â†’ Has diabetes
ğŸ§© Purpose:
Load the data youâ€™ll train the KNN model on.

ğŸ§  Step 3: Display First 5 Rows
df.head()
ğŸ“˜ Explanation:
        â€¢ Shows the first 5 records (rows) of your dataset.
        â€¢ Helps verify that the data loaded correctly and columns look right.
ğŸ§© Purpose:
Quickly preview the dataset.

ğŸ§  Step 4: Get Dataset Information
print(df.info())
ğŸ“˜ Explanation:
        â€¢ Displays column names, data types (int, float), and non-null (non-missing) counts.
        â€¢ Ensures data is clean and all columns have correct data types.
ğŸ§© Purpose:
Check for data types and missing values before preprocessing.

ğŸ§  Step 5: Check Missing Values
print("\nMissing values in each column:\n", df.isnull().sum())
ğŸ“˜ Explanation:
        â€¢ df.isnull() â†’ checks which cells have missing values (True/False).
        â€¢ .sum() â†’ counts total missing values per column.
        â€¢ Usually, diabetes dataset has no missing values, but always good to check.
ğŸ§© Purpose:
To make sure data is clean before applying the algorithm.

ğŸ§  Step 6: Separate Features (X) and Target (y)
X = df.drop('Outcome', axis=1)  # Independent variables
y = df['Outcome']               # Dependent variable
ğŸ“˜ Explanation:
        â€¢ X â†’ all input features (like Glucose, BMI, Age, etc.) â€” independent variables.
        â€¢ y â†’ target label (Outcome = 0 or 1) â€” dependent variable.
        â€¢ axis=1 â†’ means drop the column (not row).
ğŸ§© Purpose:
You need to separate input data (X) and output labels (y) before training.

ğŸ§  Step 7: Split Data into Training and Testing Sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
ğŸ“˜ Explanation:
        â€¢ Splits the dataset into:
                â—‹ 80% â†’ training data (for model learning)
                â—‹ 20% â†’ testing data (for evaluating model performance)
        â€¢ random_state=42 ensures results are repeatable (same random split each time).
ğŸ§© Purpose:
Train on one part of data and test how well it performs on unseen data.

ğŸ§  Step 8: Scale (Normalize) the Data
scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)
ğŸ“˜ Explanation:
        â€¢ KNN uses distance (Euclidean distance) between points.
        â€¢ If one column has large values (like Glucose = 100) and another has small (like BMI = 1), large ones dominate.
        â€¢ StandardScaler() standardizes all features to same scale (mean = 0, standard deviation = 1).
ğŸ§© Purpose:
To make sure all features contribute equally when calculating distances.

ğŸ§  Step 9: Create KNN Classifier
knn = KNeighborsClassifier(n_neighbors=18)
ğŸ“˜ Explanation:
        â€¢ Creates a KNN model where k = 18.
        â€¢ Means: for each test data point, the algorithm looks at the 18 nearest training points (neighbors) to decide the class (0 or 1).
        â€¢ The most common class among those 18 neighbors becomes the prediction.
ğŸ§© Purpose:
Define the model â€” KNN uses majority voting among nearest points.

ğŸ§  Step 10: Train the Model
knn.fit(X_train, y_train)
ğŸ“˜ Explanation:
        â€¢ Fits (trains) the KNN model using training data.
        â€¢ In KNN, â€œtrainingâ€ means storing all training data because itâ€™s a â€œlazy learnerâ€ â€” it doesnâ€™t build a formula, it memorizes data.
ğŸ§© Purpose:
Prepare the model so it can make predictions on unseen data.

ğŸ§  Step 11: Predict on Test Data
y_pred = knn.predict(X_test)
ğŸ“˜ Explanation:
        â€¢ Uses the trained model to predict the diabetes outcome (0 or 1) for test patients.
        â€¢ Stores predictions in y_pred.
ğŸ§© Purpose:
To test how accurately the model predicts unseen data.

ğŸ§  Step 12: Confusion Matrix
cm = confusion_matrix(y_test, y_pred)
print("Confusion Matrix:\n", cm)
ğŸ“˜ Explanation:
        â€¢ The confusion matrix shows counts of:
                â—‹ True Positives (TP) â€” correctly predicted â€œdiabeticâ€
                â—‹ True Negatives (TN) â€” correctly predicted â€œnon-diabeticâ€
                â—‹ False Positives (FP) â€” predicted diabetic but actually not
                â—‹ False Negatives (FN) â€” predicted non-diabetic but actually diabetic
Example:
[[90 10]
 [20 34]]
TN=90, FP=10, FN=20, TP=34
ğŸ§© Purpose:
To understand how the model performs in detail.

ğŸ§  Step 13: Calculate Performance Metrics
accuracy = accuracy_score(y_test, y_pred)
precision = precision_score(y_test, y_pred)
recall = recall_score(y_test, y_pred)
error_rate = 1 - accuracy
ğŸ“˜ Explanation:
Metric
Formula
Meaning
Accuracy
(TP + TN) / Total
How many predictions are correct overall
Precision
TP / (TP + FP)
Of those predicted positive, how many are actually positive
Recall
TP / (TP + FN)
Of all actual positive cases, how many were correctly found
Error Rate
1 - Accuracy
How often the model is wrong
ğŸ§© Purpose:
To measure the effectiveness of your classifier.

ğŸ§  Step 14: Display Metrics
print(f"\nAccuracy: {accuracy:.4f}")
print(f"Error Rate: {error_rate:.4f}")
print(f"Precision: {precision:.4f}")
print(f"Recall: {recall:.4f}")
ğŸ“˜ Explanation:
        â€¢ :.4f â†’ shows results up to 4 decimal places.
        â€¢ Prints all key metrics neatly for interpretation.
ğŸ§© Purpose:
To clearly show how well the model performs.

ğŸ§  Step 15: Classification Report
print('Classification Report\n', classification_report(y_test, y_pred))
ğŸ“˜ Explanation:
        â€¢ Gives a summary report including:
                â—‹ Precision
                â—‹ Recall
                â—‹ F1-score (harmonic mean of precision & recall)
                â—‹ Support (number of test samples per class)
Example:
              precision    recall  f1-score   support
0             0.85         0.93     0.89       100
1             0.77         0.61     0.68        50
ğŸ§© Purpose:
To get a full performance summary for both classes (diabetic vs non-diabetic).

ğŸ§¾ Summary of the Steps
Step
Description
1
Import libraries
2
Load dataset
3
Explore and clean data
4
Split features (X) and target (y)
5
Train-test split
6
Scale features
7
Create KNN model
8
Train model
9
Predict output
10
Evaluate with confusion matrix
11
Print accuracy, precision, recall, and classification report

ğŸ¤ Common Viva Questions (and Answers)
Question
Answer
What type of algorithm is KNN?
Itâ€™s a supervised classification algorithm.
What does â€œKâ€ mean?
Number of nearest neighbors to consider.
How does KNN classify a new point?
By taking a majority vote from its K nearest neighbors.
Why do we scale data in KNN?
Because it depends on distance; unscaled data gives wrong results.
What is the confusion matrix?
A table that shows predicted vs actual classifications.
What is precision?
Out of all predicted positives, how many are truly positive.
What is recall?
Out of all actual positives, how many were found correctly.
How do you find the best K?
Try different K values and pick the one with highest accuracy.
Is KNN a parametric or non-parametric model?
Non-parametric â€” it makes no assumptions about data distribution.
What is the main disadvantage of KNN?
Itâ€™s slow on large datasets and sensitive to irrelevant features.

âœ… Final Explanation (in Simple Words)
        â€œIn this practical, we used the K-Nearest Neighbors (KNN) algorithm to predict whether a patient has diabetes or not.
        We first cleaned and scaled the data, split it into train and test sets, trained the KNN model with k=18, and then evaluated it using metrics like accuracy, precision, recall, and confusion matrix.
        Our model achieved around 75â€“80% accuracy, showing it can reasonably predict diabetes based on medical data.â€



